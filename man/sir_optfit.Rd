% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit_optim.R
\name{sir_optfit}
\alias{sir_optfit}
\title{Fit SIR Model via Direct Optimization}
\usage{
sir_optfit(Y, W, X, Z, family, trace = 0, start = NULL)
}
\arguments{
\item{Y}{Three-dimensional array (m x m x T) of network outcomes.
Missing values (NA) are handled by excluding them from the likelihood.
Large networks (m > 100) may cause memory issues.}

\item{W}{Three-dimensional array (m x m x p) of influence covariates.
Each slice parameterizes the influence matrices. If NULL, no network
influence structure is included (reduces to standard GLM).}

\item{X}{Three-dimensional array (m x m x T) representing the network state
carrying influence. Typically lagged Y. Required if W is provided.}

\item{Z}{Four-dimensional array (m x m x q x T) of exogenous covariates.
These enter the model linearly without network interactions.}

\item{family}{Character string: "poisson", "normal", or "binomial".
Determines the likelihood and link function used.}

\item{trace}{Integer controlling optimizer output:
\itemize{
  \item 0: No output (default)
  \item 1: Final convergence report
  \item 2: Progress at each iteration
  \item 3+: Detailed debugging information
}}

\item{start}{Optional numeric vector of starting values [θ, α, β].
Length must equal q + 2p. If NULL, uses smart initialization.
Good starting values dramatically improve convergence.}
}
\value{
A list with class "sir_optim_fit" containing:
  \item{tab}{Vector of optimized parameters [θ, α, β]}
  \item{A}{The m x m sender effects matrix}
  \item{B}{The m x m receiver effects matrix}
  \item{convergence}{Convergence code from optim (0 = success)}
  \item{message}{Convergence message from optimizer}
  \item{iterations}{Number of function evaluations}
  \item{value}{Final negative log-likelihood}
  \item{hessian}{Approximate Hessian at optimum (if requested)}
  \item{gradient}{Final gradient (should be near zero)}
}
\description{
Implements direct optimization of the Social Influence Regression model using the 
Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method. This approach optimizes
all parameters simultaneously, treating the full likelihood as a single objective function.
}
\details{
Unlike the ALS method which alternates between parameter blocks, direct optimization
treats the SIR model as a single non-linear optimization problem:

\deqn{\min_{\theta, \alpha, \beta} -\ell(\theta, \alpha, \beta | Y, X, W, Z)}

where \eqn{\ell} is the log-likelihood function for the specified family.

\strong{Optimization Strategy:}
\itemize{
  \item Uses BFGS algorithm with analytical gradients computed via C++
  \item Gradients leverage the chain rule through the bilinear structure
  \item Line search ensures sufficient decrease in objective
  \item Hessian approximation updated using gradient information
}

\strong{Gradient Computation:}
The gradient with respect to parameters is:
\itemize{
  \item \eqn{\nabla_\theta \ell}: Direct gradient for exogenous covariates
  \item \eqn{\nabla_\alpha \ell}: Gradient flows through A = alpha_0 I + sum_r alpha_r W_r
  \item \eqn{\nabla_\beta \ell}: Gradient flows through B = beta_0 I + sum_r beta_r W_r
}

\strong{Initialization:}
If no starting values provided, uses smart initialization:
\enumerate{
  \item Fit GLM ignoring network effects to initialize theta
  \item Set alpha and beta to small random values or zeros
  \item Scale parameters based on outcome variance
}

\strong{Advantages over ALS:}
\itemize{
  \item Can find better global optima (not restricted to alternating updates)
  \item Faster convergence when near optimum
  \item Provides Hessian for standard error calculation
  \item Single convergence criterion
}

\strong{Disadvantages:}
\itemize{
  \item Less stable for high-dimensional problems (p large)
  \item Can fail with poor initialization
  \item Memory intensive for large networks
  \item Sensitive to scaling of covariates
}

\strong{Convergence Diagnostics:}
The optimizer reports convergence codes:
\itemize{
  \item 0: Successful convergence
  \item 1: Maximum iterations reached
  \item 10: Degeneracy of Nelder-Mead simplex
  \item 51: Warning from L-BFGS-B
  \item 52: Error from L-BFGS-B
}
}
\examples{
\dontrun{
# Small network example
m <- 10
T <- 15
p <- 2

Y <- array(rpois(m*m*T, 2), dim=c(m,m,T))
X <- array(0, dim=c(m,m,T))
X[,,2:T] <- Y[,,1:(T-1)]
W <- array(rnorm(m*m*p, sd=0.1), dim=c(m,m,p))

# Fit with direct optimization
fit_optim <- sir_optfit(Y, W, X, Z=NULL,
                        family="poisson", trace=1)
                        
# Check convergence
if(fit_optim$convergence == 0) {
  cat("Optimization successful\n")
  print(fit_optim$tab)
}

# Compare with ALS
fit_als <- sir_alsfit(Y, W, X, Z=NULL,
                      family="poisson")

# Often similar results but different paths
}
}
