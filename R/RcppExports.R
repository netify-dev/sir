# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Tensor Product for SIR Model (A * X * B')
#' 
#' @description
#' Performs the bilinear transformation central to the Social Influence Regression model.
#' Computes A * X_t * B' for each time slice t, where this product represents how
#' network influence flows through the sender effects (A) and receiver effects (B).
#' 
#' @details
#' This operation is the computational bottleneck of the SIR model, appearing in both
#' the likelihood evaluation and gradient computation. The function implements:
#' 
#' For each time t: Result[,,t] = A * X[,,t] * B'
#' 
#' Where:
#' - A captures sender-specific influence patterns (how nodes affect others)
#' - B captures receiver-specific influence patterns (how nodes are affected)
#' - X typically contains lagged network outcomes that carry influence forward
#' 
#' Mathematical interpretation:
#' - Element (i,j) of the result represents the total influence flowing from i to j
#' - This influence is mediated by the entire network structure at time t
#' - The bilinear form allows for complex, indirect influence pathways
#' 
#' Computational optimizations:
#' - Pre-computes B' once rather than for each time slice
#' - Uses Armadillo's optimized BLAS routines for matrix multiplication
#' - Memory-aware slice-wise operations to avoid large temporary matrices
#' - Compiler optimizations enabled through RcppArmadillo
#' 
#' @param X Three-dimensional array (m x m x T) representing the network state over time.
#'   Each slice X[,,t] is the network at time t that carries influence.
#'   
#' @param A Matrix (m x m) of sender effects. Element A[i,k] represents how node i
#'   is influenced by the sending behavior of node k.
#'   
#' @param B Matrix (m x m) of receiver effects. Element B[j,l] represents how node j's
#'   reception is modified by node l's receiving patterns.
#'   
#' @return Three-dimensional array (m x m x T) where element [i,j,t] represents the
#'   total bilinear influence from node i to node j at time t.
#'   
#' @examples
#' \dontrun{
#' // In R:
#' m <- 10; T <- 5
#' X <- array(rnorm(m*m*T), dim=c(m,m,T))
#' A <- matrix(rnorm(m*m), m, m)
#' B <- matrix(rnorm(m*m), m, m)
#' result <- cpp_tprod_A_X_Bt(X, A, B)
#' }
#' 
#' @note This function is called repeatedly during optimization, so efficiency is critical.
#'   The implementation avoids unnecessary memory allocations and leverages BLAS Level 3
#'   operations for optimal performance.
#'   
cpp_tprod_A_X_Bt <- function(X, A, B) {
    .Call(`_sir_cpp_tprod_A_X_Bt`, X, A, B)
}

#' Array-Matrix Product for Influence Matrices
#' 
#' @description
#' Computes a weighted sum of influence covariate matrices to construct the
#' parameterized influence matrices A or B in the SIR model.
#' 
#' @details
#' This function implements the parameterization:
#' Result = sum(k=1 to p) v[k] * W[,,k]
#' 
#' In the SIR model context:
#' - A = alpha[1] * I + sum(k=2 to p) alpha[k] * W[,,k-1]
#' - B = sum(k=1 to p) beta[k] * W[,,k]
#' 
#' The parameterization reduces the number of free parameters from O(m²) to O(p),
#' where typically p << m. This makes estimation feasible for larger networks.
#' 
#' Computational strategy:
#' - Skips zero coefficients to save computation
#' - Uses in-place addition to minimize memory allocation
#' - Leverages Armadillo's expression templates for efficiency
#' 
#' @param W Three-dimensional array (m x m x p) of influence covariates.
#'   Each slice W[,,k] represents one way nodes can influence each other
#'   (e.g., geographic proximity, social distance, shared attributes).
#'   
#' @param v Vector (p x 1) of coefficients for the linear combination.
#'   These are the parameters being estimated (either alpha or beta).
#'   
#' @return Matrix (m x m) representing the weighted combination of influence
#'   covariate matrices. This becomes either the A or B matrix in the model.
#'   
#' @examples
#' \dontrun{
#' // In R:
#' m <- 10; p <- 3
#' W <- array(rnorm(m*m*p), dim=c(m,m,p))
#' v <- c(1, 0.5, -0.3)  // Coefficients
#' A <- cpp_amprod_W_v(W, v)
#' // A is now the parameterized influence matrix
#' }
#' 
#' @note The function checks for dimension compatibility and will throw an
#'   error if v has incorrect length. Zero coefficients are detected and
#'   skipped to improve performance when the model is sparse.
#'   
cpp_amprod_W_v <- function(W, v) {
    .Call(`_sir_cpp_amprod_W_v`, W, v)
}

#' Construct Design Matrix for Alpha Updates in ALS
#' 
#' @description
#' Builds the design matrix for updating sender effects (alpha parameters) in the
#' Alternating Least Squares algorithm, holding receiver effects (beta) fixed.
#' 
#' @details
#' In the ALS algorithm, when updating alpha with beta fixed, the model becomes
#' linear in alpha. The design matrix for this GLM sub-problem has columns
#' corresponding to each influence covariate.
#' 
#' For covariate k and observation (i,j,t), the design matrix element is:
#' [W[,,k] * X[,,t] * B'][i,j]
#' 
#' Where B = sum_l beta[l] * W[,,l] is the current receiver effects matrix.
#' 
#' The algorithm:
#' 1. Compute B from current beta and W
#' 2. For each covariate k:
#'    - Calculate W[,,k] * X * B' for all time points
#'    - Flatten to match the vectorized Y
#' 3. Combine into design matrix
#' 
#' This C++ implementation is 10-100x faster than the equivalent R code using
#' loops or apply functions, making ALS feasible for larger networks.
#' 
#' @param W Three-dimensional array (m x m x p) of influence covariates.
#'   These parameterize how influence flows through the network.
#'   
#' @param X Three-dimensional array (m x m x T) of network states over time.
#'   Typically contains lagged outcomes that carry influence forward.
#'   
#' @param beta Vector (p x 1) of current receiver effect parameters.
#'   These are held fixed while updating alpha in this ALS step.
#'   
#' @return Matrix (m*m*T x p) that serves as the design matrix for GLM estimation.
#'   Each column corresponds to one influence covariate, rows match vectorized Y.
#'   
#' @note This function is called once per ALS iteration. The resulting matrix can
#'   be large (m²T x p), so memory usage should be considered for big networks.
#'   
#' @examples
#' \dontrun{
#' // Called internally by sir_alsfit during the alpha update step
#' // After computing this design matrix, the update is:
#' // glm(Y ~ -1 + cbind(Z_design, Wbeta_design), family=...)
#' }
#' 
cpp_construct_Wbeta_design <- function(W, X, beta) {
    .Call(`_sir_cpp_construct_Wbeta_design`, W, X, beta)
}

#' Construct Design Matrix for Beta Updates in ALS
#' 
#' @description
#' Builds the design matrix for updating receiver effects (beta parameters) in the
#' Alternating Least Squares algorithm, holding sender effects (alpha) fixed.
#' 
#' @details
#' In the ALS algorithm, when updating beta with alpha fixed, the model becomes
#' linear in beta. This function constructs the required design matrix efficiently.
#' 
#' For covariate k and observation (i,j,t), the design matrix element is:
#' [A * X[,,t] * W[,,k]'][i,j]
#' 
#' Where A = I + sum_l alpha[l] * W[,,l] is the current sender effects matrix
#' (with alpha[1] = 1 fixed for identifiability).
#' 
#' The algorithm mirrors the alpha update but with roles reversed:
#' 1. Compute A from current alpha and W
#' 2. For each covariate k:
#'    - Calculate A * X * W[,,k]' for all time points
#'    - Flatten to match the vectorized Y
#' 3. Combine into design matrix
#' 
#' @param W Three-dimensional array (m x m x p) of influence covariates.
#'   
#' @param X Three-dimensional array (m x m x T) of network states over time.
#' @param alpha Vector (p x 1) of current sender effect parameters.
#'   These are held fixed while updating beta in this ALS step.
#'   
#' @return Matrix (m*m*T x p) serving as the design matrix for beta GLM estimation.
#'   
#' @note The symmetry with cpp_construct_Wbeta_design reflects the bilinear
#'   structure of the model, where sender and receiver effects play dual roles.
#'   
#' @examples
#' \dontrun{
#' // Called internally by sir_alsfit during the beta update step
#' // The GLM call becomes:
#' // glm(Y ~ -1 + cbind(Z_design, Walpha_design), family=...)
#' }
cpp_construct_Walpha_design <- function(W, X, alpha) {
    .Call(`_sir_cpp_construct_Walpha_design`, W, X, alpha)
}

#' Calculate Gradient and Hessian for Direct Optimization
#' 
#' @description
#' Computes the gradient vector and Hessian matrix of the negative log-likelihood
#' for the SIR model. These are essential for gradient-based optimization methods
#' like BFGS used in the direct optimization approach.
#' 
#' @details
#' This function implements the analytical derivatives of the SIR likelihood with
#' respect to all parameters. The computation leverages the bilinear structure
#' of the model for efficiency.
#' 
#' \strong{Gradient Computation:}
#' For each parameter, the gradient accumulates:
#' \deqn{\nabla_{\cdot} NLL = \sum_{i,j,t} (\mu_{ijt} - y_{ijt}) \frac{\partial \eta_{ijt}}{\partial \cdot}}
#' 
#' Where the partial derivatives are:
#' - \eqn{\partial \eta / \partial \theta_k = Z_{ijk,t}}
#' - \eqn{\partial \eta / \partial \alpha_k = [W_k X B']_{ij}}
#' - \eqn{\partial \eta / \partial \beta_k = [A X W_k']_{ij}}
#' 
#' \strong{Hessian Computation:}
#' The Hessian has two components:
#' 1. Fisher Information (always positive semi-definite):
#'    \deqn{H_{Fisher} = \sum_{i,j,t} w_{ijt} \nabla \eta_{ijt} \nabla \eta_{ijt}'}
#'    where \eqn{w_{ijt}} is the GLM weight (variance function)
#' 
#' 2. Observed Information adjustment (for non-canonical links):
#'    \deqn{H_{Obs} = \sum_{i,j,t} (\mu_{ijt} - y_{ijt}) \nabla^2 \eta_{ijt}}
#'    This term captures the curvature from the bilinear structure
#' 
#' \strong{Identifiability Constraint:}
#' Since alpha_1 is fixed at 1 for identifiability, the function:
#' - Computes full derivatives internally
#' - Projects out the alpha_1 dimension using matrix J
#' - Returns reduced-dimension gradient and Hessian
#' 
#' \strong{Numerical Stability:}
#' - Handles missing data (NA) by skipping those observations
#' - Adds small stabilization to binomial weights to prevent singularity
#' - Uses log-space computations where possible
#' 
#' \strong{Computational Efficiency:}
#' - Pre-computes A and B matrices once per function call
#' - Reuses matrix products across gradient components
#' - Vectorized operations using Armadillo
#' - Avoids redundant calculations in inner loops
#' 
#' @param tab Parameter vector [theta, alpha_2:p, beta] of length q+2p-1.
#'   
#' @param Y Three-dimensional array (m x m x T) of observed outcomes.
#'   Can contain NA values which are automatically skipped.
#'   
#' @param W Three-dimensional array (m x m x p) of influence covariates.
#'   
#' @param X Three-dimensional array (m x m x T) carrying network influence.
#'   
#' @param Z_list List of q three-dimensional arrays (m x m x T), one per covariate.
#'   Passed as list for memory handling of 4D structure.
#'   
#' @param family String specifying distribution: "poisson", "normal", or "binomial".
#'   Determines the link function and variance structure.
#'   
#' @return List containing:
#'   \item{grad}{Gradient vector of length q+2p-1 (after identifiability projection)}
#'   \item{hess}{Hessian matrix of dimension (q+2p-1) x (q+2p-1)}
#'   \item{shess}{Score outer product matrix (for robust standard errors)}
#'   
#' @note The Hessian may not be positive definite far from the optimum, which can
#'   cause optimization issues. The score outer product (shess) provides an
#'   alternative for standard error calculation.
#'   
#' @examples
#' \dontrun{
#' // Called internally by optim() during optimization:
#' result <- cpp_mll_gH(current_params, Y, W, X, Z_list, "poisson")
#' // Use gradient for search direction
#' // Use Hessian for step size (quasi-Newton methods)
#' }
cpp_mll_gH <- function(tab, Y, W, X, Z_list, family) {
    .Call(`_sir_cpp_mll_gH`, tab, Y, W, X, Z_list, family)
}

