---
title: "Bilinear Gaussian Regression for Continuous Outcomes"
author: "s7m"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

# intro

extend to gaussian case. We assume a **normal** likelihood with identity link, plus a **rank-1 constraint** on the "influence" parameters $\alpha$ and $\beta$. As in the Poisson and binary (logistic) bilinear models, the constraint $\alpha_1 = 1$ breaks the scaling indeterminacy, and a **chain rule trick** is used to correctly compute the identified gradient and Hessian.

## setup

Let:

- $Y$ be an $(m \times m \times T)$ array of **continuous** outcomes, where $Y_{i,j,t} \in \mathbb{R}$.  
- $Z$ be an $(m \times m \times q \times T)$ array of **exogenous** dyadic covariates, so $Z_{i,j,t}\in\mathbb{R}^q$.  
- $W$ be an $(m \times m \times p)$ array that encodes **node- or dyad-level "influence"** covariates for the bilinear part.  
- $X$ be some object/array that combines, for example, lagged values $Y_{i,j,t-1}$ (or other transformations) with $W$ to form the rank-1 predictor $\alpha^\top X_{i,j,t}\beta$.

### Gaussian Assumption

We assume a **Gaussian** distribution with a fixed variance $\sigma^2=1$. Specifically,
\[
  Y_{i,j,t} \;\sim\; \mathcal{N}\bigl(\mu_{i,j,t},\,1\bigr),
\]
where the linear predictor $\mu_{i,j,t}$ is
\[
  \mu_{i,j,t}
  \;=\;
  \underbrace{\theta^\top Z_{i,j,t}}_{\text{exogenous part}}
  \;+\;
  \underbrace{\alpha^\top \bigl(\text{some function of } X_{i,j,t}\bigr)\,\beta}_{\text{bilinear rank-1 part}}.
\]

## Rank-1 Constraint and Identifiability

We have:

- $\theta \in \mathbb{R}^q$,  
- $\alpha,\beta \in \mathbb{R}^p$.  

However, because the bilinear term $\alpha^\top X \beta$ is **scale-invariant** ($\alpha\cdot c$ and $\beta\cdot (1/c)$ yields the same product), we impose the constraint:
\[
  \alpha_1 = 1.
\]
Hence, $\alpha = (1,\,\alpha_2,\ldots,\alpha_p)$. Without this constraint, there would be infinitely many $\alpha,\beta$ producing the same rank-1 structure.

## Negative Log-Likelihood

Since $\sigma^2=1$, the **negative log-likelihood** (ignoring additive constants) for one observation $(i,j,t)$ is
\[
  \mathrm{NLL}_{i,j,t}
  \;=\;
  \tfrac{1}{2} \bigl[Y_{i,j,t} - \mu_{i,j,t}\bigr]^2.
\]
Thus, for all $(i,j,t)$,
\[
  \mathrm{NLL}(\theta,\alpha,\beta)
  \;=\;
  0.5\,\sum_{i\neq j}\sum_{t=1}^T
  \bigl[Y_{i,j,t} - \mu_{i,j,t}\bigr]^2.
\]

## Gradient and Hessian (Before Imposing $\alpha_1=1$)

Let us define the "unconstrained" parameter vector:
\[
  \psi
  \;=\;
  (\theta_1,\dots,\theta_q,\;\alpha_1,\dots,\alpha_p,\;\beta_1,\dots,\beta_p).
\]
Because $\mu_{i,j,t} = \theta^\top Z_{i,j,t} + \alpha^\top X_{i,j,t}\,\beta$ is **linear** in $\psi$:

- **Gradient** w.r.t. $\psi_k$:

  \[
    \frac{\partial\,\mathrm{NLL}}{\partial\,\psi_k}
    \;=\;
    \sum_{i\neq j}\sum_{t}
    \bigl(\mu_{i,j,t} - Y_{i,j,t}\bigr)
    \;\frac{\partial\,\mu_{i,j,t}}{\partial\,\psi_k}.
  \]

- **Hessian** w.r.t. $\psi_k,\psi_\ell$:

  \[
    \frac{\partial^2\,\mathrm{NLL}}{\partial\,\psi_k\,\partial\,\psi_\ell}
    \;=\;
    \sum_{i\neq j}\sum_{t}
    \frac{\partial\,\mu_{i,j,t}}{\partial\,\psi_k}
    \;\frac{\partial\,\mu_{i,j,t}}{\partial\,\psi_\ell},
  \]
  since $\mu_{i,j,t}$ is linear in $\psi$ and has zero second derivative.

## Chain Rule Trick for $\alpha_1=1$

We do **not** estimate $\alpha_1$ directly; we **fix** $\alpha_1=1$. Hence, the "actual" or **identified** parameter vector is:
\[
  \phi
  \;=\;
  (\theta_1,\dots,\theta_q,\;\alpha_2,\dots,\alpha_p,\;\beta_1,\dots,\beta_p),
\]
so $\dim(\phi)=q + (p-1) + p = q + 2p - 1.$

### From Unconstrained to Identified

Define $\psi(\phi)$ so that:
\[
  \psi(\phi)
  \;=\;
  (\theta,\,1,\,\alpha_2,\dots,\alpha_p,\;\beta_1,\dots,\beta_p).
\]
A **Jacobian** matrix $J$ (size $(q+2p-1)\times(q+2p)$) "drops" the dimension for $\alpha_1$. The code computes:

1. The "unconstrained" gradient/Hessian in dimension $(q+2p)$.  
2. Applies $J$ to map into the identified space.  

Specifically, if $\nabla_\psi$ is the gradient in the unconstrained parameter space, $\nabla_\phi$ in the identified space is:
\[
  \nabla_\phi\,\mathrm{NLL}
  \;=\;
  -\,J\,\nabla_\psi\,\mathrm{NLL},
  \quad
  \nabla^2_\phi\,\mathrm{NLL}
  \;=\;
  -\,J\,\nabla^2_\psi\,\mathrm{NLL}\,J^\top.
\]
We also do this for the robust sandwich cross-product term, if desired.

Hence, the code "fixes" $\alpha_1=1$ yet still uses a standard linear gradient/Hessian approach under the hood, then projects out the redundant dimension.

## Final Parameter Estimates and Interpretation

After the iterative fitting procedure converges, we have:
- $\theta\in\mathbb{R}^q$,  
- $\alpha\in\mathbb{R}^p$ with $\alpha_1=1$, plus the free $\alpha_2,\dots,\alpha_p$,  
- $\beta\in\mathbb{R}^p$.  

The fitted model is:
\[
  \mu_{i,j,t}
  \;=\;
  \theta^\top Z_{i,j,t} 
  \;+\;
  \alpha^\top X_{i,j,t}\,\beta.
\]
Because it is Gaussian (identity link), the network outcome is modeled via $\mu_{i,j,t}$ directly. The "influence" matrices can be interpreted similarly to the Poisson or logistic cases, but now describing how *units* of prior interactions or covariates predict continuous responses.

## Example Code

Below is a **code chunk** demonstrating how to implement **bilinear Gaussian regression** with the rank-1 constraint $\alpha_1=1$. The functions include:

1. `mll_gH_gauss()` – Builds the negative log-likelihood, gradient, Hessian, and robust cross-product ("shess").  
2. `eta_tab_gauss()` – Computes the linear predictor array $\mu$.  
3. `mll_gaussblr()` – Computes the negative log-likelihood $\tfrac{1}{2}\sum(Y-\mu)^2$.  
4. `gaussblr_optfit()` – Fits the model via `optim(..., method="BFGS")`.  
5. `gaussblr_alsfit()` – A block coordinate descent, alternating between $(\theta,\alpha)$ and $(\theta,\beta)$ subproblems, each solved by standard linear regression in R.  
6. `se_gaussblr()` – Extracts standard errors from the Hessian (and optionally robust/sandwich).  
7. `gaussblr()` – A convenience wrapper that uses the ALS approach, returns final estimates, influence matrices, and optionally standard errors.

```{r, echo=TRUE}
#### ---- Bilinear Gaussian Regression for Continuous Outcomes ----

#### ---- 1) Gradient & Hessian
mll_gH_gauss <- function(tab, Y, W, X, Z) {
  # [See code details in the provided script...]
}

#### ---- 2) Bilinear Predictor
eta_tab_gauss <- function(tab, W, X, Z) {
  # [Compute mu = theta^T Z + alpha^T X beta]
}

#### ---- 3) Negative Log-Likelihood
mll_gaussblr <- function(tab, Y, W, X, Z) {
  # [Sum up 0.5*(Y - mu)^2]
}

#### ---- 4) Fit via 'optim'
gaussblr_optfit <- function(Y, W, X, Z, trace=0, tab=NULL) {
  # [BFGS minimization of NLL with gradient]
}

#### ---- 5) Block Coordinate Descent
gaussblr_alsfit <- function(Y, W, X, Z, trace=FALSE) {
  # [Iterate sub-OLS fits: fix beta => solve for (theta, alpha), fix alpha => solve for (theta, beta)]
}

#### ---- 6) Standard Errors
se_gaussblr <- function(tab, Y, W, X, Z, calcSE=TRUE) {
  # [Invert Hessian => classical SE; robust => sandwich]
}

#### ---- 7) Convenience Wrapper
gaussblr <- function(Y, W, X, Z, calcSE=TRUE) {
  # [ALS fit, gather results, produce final alpha,beta, log-likelihood, SEs, etc.]
}
```

**notes**:

- We **enforce** `alpha_1 = 1` with a projection matrix `J` that drops the redundant dimension from the gradient/Hessian.  
- Each sub-block update uses **ordinary linear regression** (since the link is identity) in R, analogous to the iterative weighted least squares in Poisson/logistic but simpler.  
- The final output includes the rank-1 "influence" matrices `A` and `B` if you multiply `W` by the fitted `alpha` or `beta`.

## wrapping up

This **Gaussian bilinear regression** extends the same rank-1 influence concept from Poisson or logistic settings to **continuous** outcomes. By **fixing** $\alpha_1=1$, we ensure the factorization $\alpha\beta^\top$ is **unique**, and we employ the **chain rule trick** to handle the identified gradient/Hessian dimension. Either through **block coordinate descent** (sub-OLS fits) or a **global BFGS** approach, we can efficiently fit large network data under a Gaussian assumption.